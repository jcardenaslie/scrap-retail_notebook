{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeatifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Python for Signal Processing Algorithms Implementation (Tamilnadu)', 'location': 'ECE Department Seminar Hall, IRTT, Erode, Tamilnadu, INDIA', 'time': '22 March 2020 3:30am UTC – 11:30am UTC'}\n",
      "{'name': 'PyCon SK 2020', 'location': 'Bratislava, Slovakia', 'time': '27 March – 29 March  2020'}\n",
      "{'name': 'MoscowPythonConf++', 'location': 'Moscow, Russia', 'time': '27 March 2020'}\n",
      "{'name': 'PyCon US 2020', 'location': 'Pittsburgh, PA, USA', 'time': '15 April – 23 April  2020'}\n",
      "{'name': 'Django Day Copenhagen', 'location': 'Copenhagen, Denmark', 'time': '17 April 2020'}\n",
      "{'name': 'DragonPy 2020', 'location': 'Ljubljana, Slovenia', 'time': '18 April – 19 April  2020'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_upcoming_events(url):\n",
    "    req = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "\n",
    "    events = soup.find('ul', {'class': 'list-recent-events'}).findAll('li')\n",
    "\n",
    "    for event in events:\n",
    "        event_details = dict()\n",
    "        event_details['name'] = event.find('h3').find(\"a\").text\n",
    "        event_details['location'] = event.find('span', {'class', 'event-location'}).text\n",
    "        event_details['time'] = event.find('time').text\n",
    "        print(event_details)\n",
    "\n",
    "get_upcoming_events('https://www.python.org/events/python-events/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Cookie\": \"my-cookie=browser\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.22.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e6aef78-aadf264833970f8efc77105f\"\n",
      "  }, \n",
      "  \"origin\": \"181.42.12.92\", \n",
      "  \"url\": \"http://httpbin.org/get\"\n",
      "}\n",
      "\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 0}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 1}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 2}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 3}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 4}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 5}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 6}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 7}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 8}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 9}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 10}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 11}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 12}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 13}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 14}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 15}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 16}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 17}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 18}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-5e6aef78-144788c0f5f80000a65c5f80', 'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}, 'origin': '181.42.12.92', 'id': 19}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# builds on top of urllib3's connection pooling\n",
    "# session reuses the same TCP connection if \n",
    "# requests are made to the same host\n",
    "# see https://en.wikipedia.org/wiki/HTTP_persistent_connection for details\n",
    "session=requests.Session()\n",
    "\n",
    "# You may pass in custom cookie\n",
    "r=session.get('http://httpbin.org/get',cookies={'my-cookie':'browser'})\n",
    "print(r.text)\n",
    "# '{\"cookies\": {\"my-cookie\": \"test cookie\"}}'\n",
    "\n",
    "# Streaming is another nifty feature\n",
    "# From http://docs.python-requests.org/en/master/user/advanced/#streaming-requests\n",
    "# copyright belongs to reques.org\n",
    "r = requests.get('http://httpbin.org/stream/20', stream=True)\n",
    "\n",
    "for line in r.iter_lines():\n",
    "  # filter out keep-alive new lines\n",
    "  if line:\n",
    "        decoded_line = line.decode('utf-8')\n",
    "        print(json.loads(decoded_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Python for Signal Processing Algorithms Implementation (Tamilnadu)', 'location': 'ECE Department Seminar Hall, IRTT, Erode, Tamilnadu, INDIA', 'time': '22 March 2020 3:30am UTC – 11:30am UTC'}\n",
      "{'name': 'PyCon SK 2020', 'location': 'Bratislava, Slovakia', 'time': '27 March – 29 March  2020'}\n",
      "{'name': 'MoscowPythonConf++', 'location': 'Moscow, Russia', 'time': '27 March 2020'}\n",
      "{'name': 'PyCon US 2020', 'location': 'Pittsburgh, PA, USA', 'time': '15 April – 23 April  2020'}\n",
      "{'name': 'Django Day Copenhagen', 'location': 'Copenhagen, Denmark', 'time': '17 April 2020'}\n",
      "{'name': 'DragonPy 2020', 'location': 'Ljubljana, Slovenia', 'time': '18 April – 19 April  2020'}\n"
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_upcoming_events(url):\n",
    "    req = urllib3.PoolManager()\n",
    "    res = req.request('GET', url)\n",
    "\n",
    "    soup = BeautifulSoup(res.data, 'html.parser')\n",
    "\n",
    "    events = soup.find('ul', {'class': 'list-recent-events'}).findAll('li')\n",
    "\n",
    "    for event in events:\n",
    "        event_details = dict()\n",
    "        event_details['name'] = event.find('h3').find(\"a\").text\n",
    "        event_details['location'] = event.find('span', {'class', 'event-location'}).text\n",
    "        event_details['time'] = event.find('time').text\n",
    "        print(event_details)\n",
    "\n",
    "get_upcoming_events('https://www.python.org/events/python-events/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Python for Signal Processing Algorithms Implementation (Tamilnadu)', 'location': 'ECE Department Seminar Hall, IRTT, Erode, Tamilnadu, INDIA', 'time': '22 March'}\n",
      "{'name': 'PyCon SK 2020', 'location': 'Bratislava, Slovakia', 'time': '27 March – 29 March '}\n",
      "{'name': 'MoscowPythonConf++', 'location': 'Moscow, Russia', 'time': '27 March'}\n",
      "{'name': 'PyCon US 2020', 'location': 'Pittsburgh, PA, USA', 'time': '15 April – 23 April '}\n",
      "{'name': 'Django Day Copenhagen', 'location': 'Copenhagen, Denmark', 'time': '17 April'}\n",
      "{'name': 'DragonPy 2020', 'location': 'Ljubljana, Slovenia', 'time': '18 April – 19 April '}\n",
      "{'name': 'HackBVICAM National Student’s Convention 2k20', 'location': 'New Delhi, India', 'time': '13 March'}\n",
      "{'name': 'PyCon Belarus 2020', 'location': 'Minsk, Belarus', 'time': '21 Feb. – 22 Feb. '}\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class PythonEventsSpider(scrapy.Spider):\n",
    "    name = 'pythoneventsspider'\n",
    "\n",
    "    start_urls = ['https://www.python.org/events/python-events/',]\n",
    "    found_events = []\n",
    "\n",
    "    def parse(self, response):\n",
    "        for event in response.xpath('//ul[contains(@class, \"list-recent-events\")]/li'):\n",
    "            event_details = dict()\n",
    "            event_details['name'] = event.xpath('h3[@class=\"event-title\"]/a/text()').extract_first()\n",
    "            event_details['location'] = event.xpath('p/span[@class=\"event-location\"]/text()').extract_first()\n",
    "            event_details['time'] = event.xpath('p/time/text()').extract_first()\n",
    "            self.found_events.append(event_details)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({ 'LOG_LEVEL': 'ERROR'})\n",
    "    process.crawl(PythonEventsSpider)\n",
    "    spider = next(iter(process.crawlers)).spider\n",
    "    process.start()\n",
    "\n",
    "    for event in spider.found_events: print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Python for Signal Processing Algorithms Implementation (Tamilnadu)', 'location': 'ECE Department Seminar Hall, IRTT, Erode, Tamilnadu, INDIA', 'time': '22 March 3:30am UTC – 11:30am UTC'}\n",
      "{'name': 'PyCon SK 2020', 'location': 'Bratislava, Slovakia', 'time': '27 March – 29 March'}\n",
      "{'name': 'MoscowPythonConf++', 'location': 'Moscow, Russia', 'time': '27 March'}\n",
      "{'name': 'PyCon US 2020', 'location': 'Pittsburgh, PA, USA', 'time': '15 April – 23 April'}\n",
      "{'name': 'Django Day Copenhagen', 'location': 'Copenhagen, Denmark', 'time': '17 April'}\n",
      "{'name': 'DragonPy 2020', 'location': 'Ljubljana, Slovenia', 'time': '18 April – 19 April'}\n",
      "{'name': 'HackBVICAM National Student’s Convention 2k20', 'location': 'New Delhi, India', 'time': '13 March'}\n",
      "{'name': 'PyCon Belarus 2020', 'location': 'Minsk, Belarus', 'time': '21 Feb. – 22 Feb.'}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "def get_upcoming_events(url):\n",
    "    \n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.add_argument('-headless')\n",
    "    driver = webdriver.Firefox(firefox_options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    events = driver.find_elements_by_xpath('//ul[contains(@class, \"list-recent-events\")]/li')\n",
    "\n",
    "    for event in events:\n",
    "        event_details = dict()\n",
    "        event_details['name'] = event.find_element_by_xpath('h3[@class=\"event-title\"]/a').text\n",
    "        event_details['location'] = event.find_element_by_xpath('p/span[@class=\"event-location\"]').text\n",
    "        event_details['time'] = event.find_element_by_xpath('p/time').text\n",
    "        print(event_details)\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "get_upcoming_events('https://www.python.org/events/python-events/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhantomJS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\phantomjs\\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead\n",
      "  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Python for Signal Processing Algorithms Implementation (Tamilnadu)', 'location': 'ECE Department Seminar Hall, IRTT, Erode, Tamilnadu, INDIA', 'time': '22 March 3:30am UTC – 11:30am UTC'}\n",
      "{'name': 'PyCon SK 2020', 'location': 'Bratislava, Slovakia', 'time': '27 March – 29 March'}\n",
      "{'name': 'MoscowPythonConf++', 'location': 'Moscow, Russia', 'time': '27 March'}\n",
      "{'name': 'PyCon US 2020', 'location': 'Pittsburgh, PA, USA', 'time': '15 April – 23 April'}\n",
      "{'name': 'Django Day Copenhagen', 'location': 'Copenhagen, Denmark', 'time': '17 April'}\n",
      "{'name': 'DragonPy 2020', 'location': 'Ljubljana, Slovenia', 'time': '18 April – 19 April'}\n",
      "{'name': 'HackBVICAM National Student’s Convention 2k20', 'location': 'New Delhi, India', 'time': '13 March'}\n",
      "{'name': 'PyCon Belarus 2020', 'location': 'Minsk, Belarus', 'time': '21 Feb. – 22 Feb.'}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "def get_upcoming_events(url):\n",
    "    \n",
    "    driver = webdriver.PhantomJS('phantomjs')\n",
    "    driver.get(url)\n",
    "\n",
    "    events = driver.find_elements_by_xpath('//ul[contains(@class, \"list-recent-events\")]/li')\n",
    "\n",
    "    for event in events:\n",
    "        event_details = dict()\n",
    "        event_details['name'] = event.find_element_by_xpath('h3[@class=\"event-title\"]/a').text\n",
    "        event_details['location'] = event.find_element_by_xpath('p/span[@class=\"event-location\"]').text\n",
    "        event_details['time'] = event.find_element_by_xpath('p/time').text\n",
    "        print(event_details)\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "get_upcoming_events('https://www.python.org/events/python-events/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
